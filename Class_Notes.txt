Edu_MLOps Certification Training

Orientation Day : 24th Jan. 2026 - 1Hr.

Actual Sessions start data : 31st Jan. 2026
Time: 7AM-10AM(IST) - (SAT-SUN)

#########################
Day - 0 | 24th Jan 2026
#########################
	
	- LMS Portal 
	
	- Pre-requisites :
	
		--> AWS Cloud Platform 		# Create an AWS Free Tier Account 
		--> Github Access 
		--> DockerHub Access 
		--> Visual Studio Code - Create the Python Scripts
		
	- Pre-requisites :
	
		--> Git,Docker,Kubernetes ==> 

		--> Github-Actions
		
		
#########################
Day - 1 | 31st Jan. 2026
#########################

	MLOPs Essentials :::
	
	Lab Requirements :::
	
	Pre-requisites :::
	
	What is MLOps ???
	
	What is DevOps ???
	
	SDLC ? Software Development Life Cycle!
	
	Types of Software Applications?
	
		- Web Applications
		- Mobile Applications
		- Desktop Applications
		- Embedded Applications 
		- ML Applications
		
	Stages of SDLC :
	
		- Requirements Analysis 
		- Design/Documentation
		- Code/Development
		- Test
		- Implement to Prod 
		- Monitor/Maintain
	
	Waterfall Model :
	
		- It is based on Top-Approach
		- It used Linear path to engage all the stages of SDLC
		- Waterfall Model was meant for Monolith Application Architecture
			- Tightly Coupled
		
		
		
	
		- Requirements Analysis 
		- Design/Documentation
		- Code/Development
		- Test
		- Implement to Prod 
		- Monitor/Maintain	
	
	Eg.: Desktop Applications: SuperMarket Billing System 
	
		using Waterfall Model:
		
			SuperMarket Billing System :
				- Stock Maintain
				- Application User Interface Design 
				- Billing 
				- Payment 
					- CASH/CARD 
					- Online Payment/UPI		New Change

	Core Project:
	
		- Requirements Analysis 		--> 12 Months 
		- Design/Documentation
		- Code/Development							5th Month 
		- Test
		- Implement to Prod 
		- Monitor/Maintain		
	
	
	Enhancement Project:
	
		- Requirements Analysis 		--> 12 Months 
		- Design/Documentation
		- Code/Development							5th Month 
		- Test
		- Implement to Prod 
		- Monitor/Maintain	
	
	
	AGILE Methodologies :
	
		--> Here the functions/Modules are defined as Iterations.
		--> Each Iteration will be independently analysed and implemented.
		--> Supports Monolith & Micro-Service based Application Architecture
		--> It based on Task based approach.

		
	Stages of SDLC :

	Eg.: Desktop Applications: SuperMarket Billing System 
	
		using AGILE Methodologies:
		
			SuperMarket Billing System :
				- Stock Maintain
				- Application User Interface Design 
				- Billing 
				- Payment 
					- CASH/CARD 
					
					- Online Payment/UPI		New Change

	Iteration 1 : Stock Maintain
	
		- Requirements Analysis 
		- Design/Documentation
		- Code/Development
		- Test
		- Implement to Prod with proper approvals
	
	
	Iteration 2 : Application User Interface Design 
	
		- Requirements Analysis 
		- Design/Documentation
		- Code/Development
		- Test
		- Implement to Prod with proper approvals	
	
	
	Iteration 3 : Payment 
	
		- Requirements Analysis 
		- Design/Documentation
		- Code/Development
		- Test  100% 
		- Implement to Prod with proper approvals	
	
	
	Iteration nth : Online Payment/UPI
	
		- Requirements Analysis 
		- Design/Documentation
		- Code/Development
		- Test
		- Implement to Prod with proper approvals	
		
		
		
		- using AGILE Methodologies, we can able achieve :
		
			- Continuous Development
			- Continuous Integration 
			- Continuous Testing 
			- Continuous Delivery 
				- It is a process to release to changes to Prod Enviroment, This process expects Manual approvals for Prod release.
				
			
		- But, using AGILE Methodologies, we cannot achieve :
		
			- Continuous Deployment 
				- It is a process to release to changes to Prod Enviroment, This process Never expects any Manual intervension/approvals for Prod release.
				
				
	- Devops :::::
	
	- What is DevOps ???	

		--> DevOps is Software Development Strategy, which helps to promote the collaboration between the teams like Development Teams and Operations Teams in order to achieve Continuous Development, Continuous Integration, Continuous Testing, Continuous Delivery, Continuous Deployment and Continuous Monitoring in more automated fashion. 
		
	- How to Implement DevOps ???
	
		--> Identify the Teams involved in the overall SDLC process :::
		
			- DevOps Team ==> 
			
				- Infra-Structure Management Team
				- Application Development Team 
				- Testing Team 
				- Release Management Team 
				- Production Support Team 
				- Production Monitoring Team 
				- IT Security Team 			
				
			- ALM - Application LifeCycle Management Tool - Jira/AzBoard 
			
			-> Onboard Applications to DevOps :
			
				-> Detailed DevOps Assessment!
				
					- Recommendations --> 
					
			
		--> Identify the Enviroments :::
		
			Non-Prod Enviroment					Prod Enviroments
			
				- Dev 
				
				- Build 
				
				- Test 
				
					- QA 
					
					- UAT 		============>		Prod_Servers 
				
				
		--> DevOps Stages :::
			
			- Continuous Development 
				-> It is the capability of any development team to continuously develop the code.
				-> In DevOps this Continuous Development Stage is used to Improve the Developers' Productivity.
				
				Role of Developer : 

					- Understand the Design 
					- Create the Source Code 
					
				Developer :
				
					- Create Source Code 	--> java/python
					- Build Source Code 	--> Compile 
					- Create Application Artifacts - Binary(*.exec/*.dll/*.war)
					- Perform unit Testing 
					- Promote the Changes to Test Enviroments
					- Notify the Testing Team 
					
				Using DevOps :
				
					- Create Source Code 	--> java/python
					- Save the source Code in the Code Repository(github)
					
				Automate :
				
					- Application Build 
					- Create Artifacts 
					- Automated Unit Testing 
					- Code Promotion 
					- Automate Notification 
					
				
				Tools :
				
					- IDE - Integrated Development Enviroment - Eclipse based IDE - Visual Studio, Visual Studio Code, Pycharm, Anaconda IDEs 
					- Git/Github
					
					Visual Studio Code.
					GitHub Source Code Management
			
			- Continuous Integration :
			
				- It is the capability of any development team to continuously integrate the their changes to higher environment for further testing
				
				Automate :
				
					- Application Build 
					- Create Artifacts 
					- Automated Unit Testing 
					- Code Promotion 
					- Automate Notification 
						
				Tool: 
					- IDE - Integrated Development Enviroment - Eclipse based IDE - Visual Studio, Visual Studio Code, Pycharm, Anaconda IDEs 
					- Git/Github
					
					- Build Tools 
					
					- Docker / Kubernetes / Ansible 
					
					- Jenkins/Github-Action/Azure Pipelines
					
			
			- Continuous Testing ::

				- It is the capability of any testing team to continuously test the changes without waiting/impacting other changes.
				
				Tools :
				
					- Junit/TestNG/Selenium 
					- Jenkins/Github-Action/Azure Pipelines
				
				
			- Continuous Delivery/Deployment ::
										
				- Application Architecture ::: (DevOps Perspective)
				
					- Monolith Application Architecture
						- Tightly Coupled
						- All the dependencies are coded together  like the legacy application 
						- It is not that easy to split this to Micro-Services.
						- We can achieve Continuous Delivery. Not Continuous Deployment.
					
					- Micro-Service based Application Architecture 
					
						- Here the functions/Modules are loosely coupled. 
						- Each and every function are called as Micro-Service
						- Each Micro-Service can be independently developed, tested and Implemented to Prod.
						- To achieve Continuous Deployment, it should be Micro-Service based Architecture.						
						
				- Continuous Delivery 
					- It is a process to release to changes to Prod Enviroment, This process expects Manual approvals for Prod release.

				
						Eg,: 
						
							Online Banking Services : Monolith 
							
								-> Downtime ==> Production Release Window 
												-> 4Hrs - 6Hrs
												
								-> New_Version --> Fix the Issue / Revert the changes 
								
					
				- Continuous Deployment 
					- It is a process to release to changes to Prod Enviroment, This process Never expects any Manual intervension/approvals for Prod release.
					- Deploy the Changes to Production without any downtime using Deployment Strategies.				
									
				Tools :				
				
					- Docker/Kubernetes 
					- Jenkins/Github-Action/Azure_Pipelines
					- Ansible 
				
				
					Eg: facebook/Netflix/amazon : Micro-Service based Application Architecture
				
				CI/Cd(delivery) & CI/CD(Deployment)
				
				
			www.amazon.com
				
				
			Sign_Up 	--> Developer1 -> Create Source Code --> Test --> promote for further testing(QA) -> Release to prod.
			Sign_In		--> Developer2 -> Create Source Code --> Test --> promote for further testing(QA) -> Release to prod.
			Search 		--> Developer2 -> Create Source Code --> Test --> promote for further testing(QA) -> Release to prod.
			Add to Cart 
			Place Order 
			Payment 
			Confirm Order 
			Track 
				
			
			- Continuous Monitoring ::
			
				- It is used to achieve Business Continuity
				- To ensure High Availability of Production Infra-Structure and Application Services. 
		
		
				- Infra-Structure 	--> Prod Servers and Resources 
				
				- Application Services 
			
		
				- Monitoring Tools :
				
					- Infra-Structure Monitoring Tools :
					
						- Prometheus, Grafana, Dynatrace, splunk, Mlflow 
						- AWS CloudWatch
						
					- Application Monitoring Tools :
					
						- DataDog, AppDynamics
						
						- Micro-Services --> Database 
					
					
		SDLC: Waterfall -> AGILE -> DevOps --> MLOPs --> LLMOps --> AIOps --> GitOps --> SRE --> ITOps
		
		Continuous Learning! Upskill 50/50 ==> 
		
		
		Tools :: DeveOps/MLOps 
		
		- Open-Source Tools 
			- git/github/github-actions/jenkins/docker/kubernetes/mlflow/streamlit/fast-api
			
		- Managed Services :
		
			- AWS	- AWS Sagemaker-ai 
				
			- Azure - Az ML 
			
			- GCP 	- Google Vertix
			
			
	
		--> Pre-requisites :
		
		
			--> AWS Cloud --> Create VMs 
			
			--> github Account 
			
			--> github-actions 
			
			--> VS Code 
			
			--> git 
		
		
		--> Version Control System 
		
		--> Docker 
		
		--> Kubernetes
		
		--> MLOps Tools Stack 
		
		--> AWS SageMaker-ai

		--> Project 
		
			
		
#########################
Day - 2 | 1st Feb. 2026
#########################			
			
	Continue with MLOps Essentials 

	Pre-requisites 
	
	Version Control System using github / S3 


	DevOps Workflow :	Application Projects --> jenkins/github-actions
		CICD :
		
		-- SCM_Checkout		--> Application Source Code 
		-- Build Application 
			- Create Application Artifacts - (*.exec/*.dll/*.war)
		-- Application Image Build using Docker 
		-- Publish to Docker Registry
		-- Deploy to Target Environment using Kubernetes
	
	MLOps Workflow :	--> AI/ML Project	--> github-actions
		CICD :
		
		-- SCM_Checkout		--> MLProject Source Code 
		-- Build Application 
			- Web Application
			- APIs
		-- Docker 	- Containerize 
		-- Feature Pipeline: Identify the Dataset, Process the Data, Train the Model
		-- Test & Evaluate 
		-- Deploy to Prod
		
	
	Version Control System using github / S3 :::
	
	
		-- github --> Version Control the Application Source --> 
		
		-- AWS S3 Buckets --> Version Control the Datasets and MetaData --> 	
		
		
		-- Access to AWS	
		
			https://aws.amazon.com/console/
			https://aws.amazon.com/resources/create-account/
			
				- Create free Tier Account --> 6 Months - 12 Months
				- $200 
				- Active Email_ID/Mobile_Number/Credit/Debit Card
				- Email_ID is the root user ID to login to AWS Console
		
		-- Access to Github & DockerHub 
				- https://github.com/
				
				- https://hub.docker.com/
				
				- https://github.com/Edu-MLOps-Feb26/Training_Documents
				
				
		
		-- Install Visual Studio Code
				- https://code.visualstudio.com/download
				
		-- Install Git Client on Local Machine
		
			
	- Version Control System using git ::::
	
	- What is Version Control System :::
	
		-> Used to Version control the Source Code Changes 
		-> To Track the Source Code Changes
		

	-> MLProject 
	
		- Web Application
		- APIs
		
	-> Python Program 
	
		mywebapp.py 
		
		--
		-
		-
		-
		-
		-
		
		save as mywebapp.py 
		
		test the program 	-- Logical Unit of Task 
		
		mywebapp.py 
		
		--
		-
		-
		-
		-
		-
		asdf
		asdfas
		dfa
		sfd
		asdf
		
		save as mywebapp.py 				mywebapp.py_v1.0
		
		
		test the program 	-- Logical Unit of Task 
		

		mywebapp.py 
		
		--
		-
		-
		-
		-
		-
		asdf
		asdfas
		dfa
		sfd
		asdf
		
		save as mywebapp.py 				mywebapp.py_v1.1
		save as mywebapp.py 				mywebapp.py_v1.2		
		save as mywebapp.py 				mywebapp.py_v1.3
		save as mywebapp.py 				mywebapp.py_v1.4
		save as mywebapp.py 				mywebapp.py_v1.5
		
		test the program 	-- Logical Unit of Task		
		
		
	-> Install git client on Local Machine 
	
	-> GIT ::
	
		-> Git is a open-source Distributed Version Control System
		-> Used to Version control the Source Code Changes 
		-> To Track the Source Code Changes	
		-> Used to perform Parallel Development using Git Branching Techniques
		
	-> Working with GIT :
	
		- Install git client on Local Machine 
		- git (vs) github
		- Core git Concepts 
		
	
	Versioning:
	
		Source_Code Changes 	:	Application Artifacts		:		Application Images 		:		Datasets for ML Model
		
		GITHUB					:	Jfrog						:		DockerHub 				:		S3  
		
		
	GIT ::
	
		What is Distributed VCS ?
		
		- Enhancement/Bugfix Project
		- New Product development
		
		
		GIT Workflow ::
		
		Local Machine 																				Remote Server
		
		Install git client 
		
		Working Directory			--> Staging Area 			--> 	Local Repository				Remote Repository
		
		mywebapp.py		  ------------->  mywebapp.py  -------------->	mywebapp.py_v1.0 	----------->  mywebapp.py_v1.0 
							git add 					git commit 							 git push
		mywebapp.py		  ------------->  mywebapp.py  -------------->	mywebapp.py_v1.1 	----------->  mywebapp.py_v1.1 
							git add 					git commit 							 git push
		mywebapp.py		  ------------->  mywebapp.py  -------------->	mywebapp.py_v1.2 	----------->  mywebapp.py_v1.2 
							git add 					git commit 							 git push
		
		
		Git Cli Commands :::
			
			git clone 		-> To Copy/Clone the entire remote repository to Local Machine
			
			git add 		-> To Add the changes from working directory to staging area 
			
			git commit 		-> To Commit the changes from staging area to local repository
			
			git push		-> To Push the changes from local Repository to Remote Repository
		
			git fetch/pull :
			
				-> Both Git fetch and pull are used handle the incremental changes from remote repository.
				
				Git fetch :
				
					- It is used to just check for the incremental changes from remote repository, if any incremental changes exist, it will update the details of the changes only in local repository. Git fetch will never Update the Working Directory.
					
					- Do explicit merge to update the Working Directory					
					
			
				git Pull :
				
					- It is used to check for the incremental changes from remote repository, if any incremental changes exist, it will update the details of the changes in local repository as well as in the Working Directory.
					
					git pull => git fetch + git merge 
					
			
			fork 			->	To Copy the one remote repository to another remote repository	
			
			
			git init 		-> 	To Initialize the Local Git Repository 
								It will create .git - GIT! Directory
								It will create a default branch - master/main 
								
								
	-> Install git client on Local Machine :::
	
		https://git-scm.com/install/windows		- Git for Windows/x64 Setup.
		
			-> git bash 		***
			
			-> git CMD
			
			-> git GUI
			
			
			Mac/Linux Open Terminal 
			
				git --version 
				
			Project Workspace Directory:
			
			
				cd d:
				mkdir Edu_MLOps_Feb26_Projects
				cd Edu_MLOps_Feb26_Projects
				mkdir Repo1
				mkdir Repo2 
				cd Repo1 
					git init 
				
				
		
		https://git-scm.com/install/linux
		
		
		- Create and work with Local Repositories
		
			- git init 
		
		
		Local Machine 																				Remote Server
		
		Install git client 
		
		Working Directory			--> Staging Area 			--> 	Local Repository				Remote Repository
		
		file1.txt		  ------------->  file1.txt  -------------->	 file1.txt 	----------->   
							git add 					git commit
		
		
		
		git init 
		
		
		git status 		--> get the current status of repository
		
		ls 				--> Linux Command to get the list of files and folders from Working Directory
		
		git ls-files 	--> Git Cli Command to get the list of files and folders that are being tracked by git.
		
		git log 
		
		
		
		
		Git Configurations :::
		
		- git Global Configuration 
		
			- As a best practice, before making a very initial commit setup the user name and email_id using Global Configuration.
			
			
			git config --global user.name "Loksai"
			
			git config --global user.email "Loksai@adf.com"			
			
		
		One time activity:
		
				cd d:
				mkdir Edu_MLOps_Feb26_Projects
				cd Edu_MLOps_Feb26_Projects
				mkdir Repo1 
				cd Repo1 
					git init
						git config --global user.name "Loksai"						
						git config --global user.email "Loksai@adf.com"							
						echo "rec1" >> f1.txt
						git add f1.txt 						
						git commit -m "Created f1.txt"
						
						
		git add ::
		
			git add fi1e1.txt 
			
			git add file1.txt file2.txt 
			
			git add s1.java 
			
			git add *.py 
			
			git add .								# '.' denoted all the files
					
		
		
		Unstage:
		
			git rm --cached f1.txt 			# Remove the changes from staging area and take the changes back to working directory
		
			git rm -f f1.txt				# Permanently remove the file
		
		
Next :

			Commits
			
			.gitignore 
			
			branches 
			
			Remote Repos 
			
			
  501  cd d:
  502  git --version
  503  mkdir Edu_MLOps_Feb26_Projects
  504  cd Edu_MLOps_Feb26_Projects
  505  pwd
  506  mkdir testrepo1
  507  cd testrepo1/
  508  pwd
  509  clear
  510  ls -a
  511  git init
  512  ls
  513  ls -a
  514  cd .git/
  515  ls
  516  cd ..
  517  clear
  518  ls
  519  git status
  520  echo "rec1" >> f1.txt
  521  git status
  522  ls
  523  git ls-files
  524  git add f1.txt
  525  git status
  526  git ls-files
  527  git commit -m "Created file.txt"
  528  git status
  529  ls
  530  git ls-files
  531  git log
  532  ls
  533  echo "rec1" >> file2.txt
  534  git status
  535  git add file2.txt
  536  git status
  537  git commit -m "Created file2.txt"
  538  git log
  539  cd ..
  540  ls
  541  ls
  542  mkdir testrepo2
  543  cd testrepo2
  544  clear
  545  git init
  546  echo "rec1" >> f1.txt
  547  git add f1.txt
  548  git commit -m "Created f1.txt"
  549  clear
  550  ls
  551  git log
  552  ls
  553  echo "rec1" >> s1.txt
  554  echo "rec1" >> s2.txt
  555  echo "rec1" >> s3.txt
  556  echo "rec1" >> q1.java
  557  echo "rec1" >> q2.java
  558  echo "rec1" >> q3.java
  559  echo "rec1" >> a1.py
  560  echo "rec1" >> a2.py
  561  echo "rec1" >> a3.py
  562  echo "rec1" >> w1.doc
  563  echo "rec1" >> w2.doc
  564  echo "rec1" >> w3.doc
  565  clear
  566  ls
  567  git status
  568  git add a1.py
  569  git status
  570  git add q1.java s1.txt
  571  git status
  572  git add *.java a2.py
  573  git status
  574  git add .
  575  git status
  576  git rm --cached a1.py
  577  git status
  578  git add .
  579  git status
  580  ls
  581  git ls-files
  582  git rm -f a1.py
  583  ls
  584  git ls-files
  585  history
		
		
		
		
		
		
		
		
		GITHUB Workflow 
		
		GIT Cli Commands 
		
		Git Branching Techniques & Strategies
		
		Remote Git Repository using GITHub




			
		
#########################
Day - 3 | 7th Feb. 2026
#########################			
		
	VCS :

		Continue with GIT & GITHUB 
		
		AWS Cloud Platform 
			- aws cli 
			- S3 Service 
			
		Containerization using Docker : Docker 
	
	
	- Continue with GIT & GITHUB	
		
		- git commit  :::
		
		
			git commit -m "Created f1.txt"
			
			Release_ID 
			
			Change Request Number
			
			Commit Message: 
			
				git commit -m "CR0226-Updated payment module"
			
		
		- git IGNORE!::
		
			- It is used to ignore the files from tracking
			- create .gitignore - maintain all the files to be ignored.
			- As a best practice, .gitignore should be the very first commit.
			
			- In MLOPs Workflow, the Source Datasets should not be commited to GITHUB Repo.
			
			- Application Source -> github 
					
			- Datasets			 -> s3 
			
			
		Eg.::
		
			MLProject:
			
				ML_Project_Dir
					- app.py 
					- app.properties 
					- db_secret.json
					- dataset
		
		
		- git log :::
		
			git log 
			
			git log --oneline 			# get short commit id 
			
			git log -3
			
			git log --oneline -3
		
		
		
		- git show <commit_id>
		
		
		
		- Undo the Committed the Changes 
		
			- git reset 
			- git revert 
			
		
		- git RESET :::
		
			- Git Reset is sed to Undo the committed changes 
			- It used to take the HEAD Pointer back to previous commit point 
			- git reset will never create any new commit point for tracking 
			- So, it is not recommended to use git reset in the Shared Repositories
			
			
			git reset <reset_option> <previous_Commit_ID>
	
			
			git reset Options :::
			
				- git reset --soft <previous_Commit_ID>
				
					- To reset the HEAD Pointer to Previous Commit point
					- Take the Changes back to Staging Area from local repository 
					- The Changes exist in staging area as well in the working directory
					
				- git reset --mixed <previous_Commit_ID>									# Default
				
					- To reset the HEAD Pointer to Previous Commit point
					- Take the Changes back to working directory from local repository 
					- The Changes exist only in the working directory								
			
				- git reset --hard <previous_Commit_ID>
				
					- To reset the HEAD Pointer to Previous Commit point
					- The files/Changes will be permanently removed from Local repository, Staging Area and Working Directory 
		
			
			
		- GIT Revert :::
		
			- Git Revert is same as git reset --hard.
			- Git Revert will create a commit point for tracking purpose
			- Git Revert is used to revert any specific commit point.
			- Hence, it is recommended in Shared Repositories
			
			
			git revert <specific_commit_id>
				
				
			Scenario :::
			
				ML Project ::
				
				
				Automate: --> every 30mins
				
				ETL : Extract, Transform & Load  --> Database/Datasets
				
				
				Repo1:
					
					cm2 ==> Extracted metadata	-->  Transform --> Load to Database --> Reset --hard <previous_Commit_ID>
					cm1 ==> .gitignore
					
				
				Repo1: 
				
					cm1 ==> .gitignore			
				
			
			
		- git branching techniques 
		
			- GIT Branch is a logical copy of a Repository(main_Branch)
			
			- To implement parallel development.	
			
			
		- GIT Branching Strategies :::
		
			- It is mainly used to ensure the integrity of the Main/Master Branch(Prod Version of Source)
			- Git Branches are used work on the Repository without impacting the main_Branch
			
			
		Scenario1: 
		
		Repo1: 	
		
			main : cm1,cm2,cm3 
					: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3			# Upon merging changes from feature_branch1
			
				feature_branch1 : cm1,cm2,cm3 
				
								: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3
	
		
		Scenario2: 
		
		Repo1: 	
		
			main : cm1,cm2,cm3 
					: cm1,cm2,cm3,cm4						# cm4 is the changes from Developer_Branch
			
				Developer_Branch : cm1,cm2,cm3 		
									cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
				
					feature_branch1 : cm1,cm2,cm3 
					
									: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

					feature_branch2 : cm1,cm2,cm3 
					
									: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3
									
									
		
		Scenario3: 
		
		Repo1: 	
		
			main : cm1,cm2,cm3 
					: cm1,cm2,cm3,cm4						# cm4 is the changes from Integration_Branch
					
				Integration_Branch : cm1,cm2,cm3 
									
									: cm1,cm2,cm3,Developer1_Changes,Developer2_Changes
						
				
					Developer_Branch1 : cm1,cm2,cm3 		
										cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
					
						feature_branch1 : cm1,cm2,cm3 
						
										: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

						feature_branch2 : cm1,cm2,cm3 
						
										: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3


					Developer_Branch2 : cm1,cm2,cm3 		
										cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
					
						feature_branch1 : cm1,cm2,cm3 
						
										: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

						feature_branch2 : cm1,cm2,cm3 
						
										: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3


		Scenario4: 
		
		Repo1: 	
		
			main : cm1,cm2,cm3 
					: cm1,cm2,cm3,cm4						# cm4 is the changes from Release_Branch
					
				Release_Branch: cm1,cm2,cm3,App_Team1_Changes,App_Team2_Changes
					
					Integration_Branch1 : cm1,cm2,cm3 														# App_Team1
										
										: cm1,cm2,cm3,Developer1_Changes,Developer2_Changes
							
					
						Developer_Branch1 : cm1,cm2,cm3 		
											cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
						
							feature_branch1 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

							feature_branch2 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3


						Developer_Branch2 : cm1,cm2,cm3 		
											cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
						
							feature_branch1 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

							feature_branch2 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3


					Integration_Branch2 : cm1,cm2,cm3 														# App_Team2
										
										: cm1,cm2,cm3,Developer1_Changes,Developer2_Changes
							
					
						Developer_Branch1 : cm1,cm2,cm3 		
											cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
						
							feature_branch1 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

							feature_branch2 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3


						Developer_Branch2 : cm1,cm2,cm3 		
											cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4,f2cm1,f2cm2,f2cm3
						
							feature_branch1 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f1cm4

							feature_branch2 : cm1,cm2,cm3 
							
											: cm1,cm2,cm3,f2cm1,f2cm2,f2cm3






	Working with Git Branches ::

		 git branch 						:	To List the Branches 
		 
		 git branch <new_branch_name>		: 	To Create New Branch
		 
		 git switch -c <new_branch_name>	: 	Create and Switch to New Branch

		 git switch <existing_branch_name>	: 	Switch to a Branch
	 
	 
	 git init 
	 
	 git commit 
	 
	 git switch -c feature1 
	 
	 Repo1: 
	 
		master : cm1 
		
			feature1 : cm1
	 
						: cm1,f1cm1 

					git switch master 
					
					git merge feature1 					# To Merge changes from feature1 
														# Execute the Merge Commmand from the Target Branch 
					
	 
	 
	 Git Merge Conflicts :::
	 
		--> Merge Conflict occurs when more than one feature/user try to update the same file and record in the target branch.
	
	
	How to Fix the Merge Conflict ???
	
		--> Identify the file(s) causing the conflicts
		
		--> Open and Review the file content and decide, which feature changes to be retained.
		
		--> Update the file content: Remove the additional header and footer tags and update the records & Save
		
		--> Perform git add and Commit the Changes to the Target Branch
		
		
	On DevOps/MLOps perspective, we never fix the merge conflict.
		- We always try to prevent the conflicts.
		
		
		
	Branches :::
	
		GIT Rebase :::
		
			--> It is used to maintain linear commit history 
			--> It is used to keep the current branch in-sync with the target branch 
			--> It is used to maintain the integrity of target branch / Prevent Merge Conflict in Target Branch
			--> As a best practice, it is always recommended to use git rebase before merge!
			
			
		Repo1:
		
			master: cm1,cm2,cm3 
			
					cm1,cm2,cm3,f1cm1,f1cm2,f1cm3
					
					cm1,cm2,cm3,f2cm1,f2cm2,f2cm3,f1cm1,f1cm2,f1cm3					# Upon merging feature2 without proper rebase
			
					cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f2cm1,f2cm2,f2cm3					# Expected Commit History
				
				feature1 : cm1,cm2,cm3 
							cm1,cm2,cm3,f1cm1,f1cm2,f1cm3
							
	
			
			
			
				feature2 : cm1,cm2,cm3 			
				
							cm1,cm2,cm3,f2cm1,f2cm2,f2cm3
							
							git rebase master 
							
							cm1,cm2,cm3,f1cm1,f1cm2,f1cm3,f2cm1,f2cm2,f2cm3
							
							git switch master 
							
							git merge feature2
		
		
		
		git SQUASH ::::
		
			git merge --squash :
			
				- It is used to combine more than once commit points into single commit point during merge.
				
				
				Repo:
				
					master : cm1 
					
						feature1: cm1 
									cm1,f1cm1,f1cm2,3,4,5,6,7,8,9,............,110
	
	
							git switch master 
							
							git merge --squash feature1 
							
								-- git commit -m "Updated multiple changes from feature1"
		
		
		
		
	git Cherry-pick :
		
		- TO Merge a specific commit to the target branch 
		- It is not recommended in real time.
	
	
		Repo: 
			master: 
					cm1
					
				feature1: 
					
					f1cm2
					f1cm1
					cm1 
	
			git cherry-pick <commit_id1> <commit_id1>
			
			
			
	
	- git stash :::
		
		 - Used to maintain the uncommited changes to a temporary area.
		 
		 - Stash is List Data Structure 
		 
		 
		 
		 git stash list 
		 
		 git stash save "create from w1.txt"
		 
		 git stash apply 		# Apply the latest changes back to staging area 
		 
		 git stash drop 		# Delete the latest changes from the stash list
		 
		 git stash pop 			# Apply and delete the changes
		 
		 
Next :::

		  501  cd d:
  502  cd Edu_MLOps_Feb26_Projects
  503  ls
  504  mkdir sapmplerepo1
  505  rm -rf sapmplerepo1/
  506  mkdir samplerepo1
  507  clear
  508  ls
  509  cd samplerepo1/
  510  git init
  511  echo "rec1" >> f1.txt
  512  git add .
  513  echo "rec2" >> f1.txt
  514  git add .
  515  git commit -m "Created f1.txt"
  516  cd ..
  517  ls
  518  clear
  519  mkdir samplerepo2/
  520  clear
  521  cd samplerepo2/
  522  clear
  523  git init
  524  ls
  525  ls -a
  526  git status
  527  vi .gitignore
  528  git status
  529  git add .
  530  git commit -m "Initial Commit - gitignore"
  531  clear
  532  ls
  533  ls -a
  534  cat .gitignore
  535  ls
  536  echo "rec1" >> file1.txt
  537  git status
  538  echo "rec1" >> sample.txt
  539  ls
  540  git status
  541  clear
  542  git log
  543  echo "rec1" >> q1.txt
  544  git add .
  545  git commit -m "created q1.txt"
  546  echo "rec1" >> q2.txt
  547  git add .
  548  git commit -m "created q2.txt"
  549  echo "rec1" >> q3.txt
  550  git add .
  551  git commit -m "created q3.txt"
  552  echo "rec1" >> q4.txt
  553  git add .
  554  git commit -m "created q4.txt"
  555  echo "rec1" >> q5.txt
  556  git add .
  557  git commit -m "created q5.txt"
  558  clear
  559  git log
  560  git log
  561  git log --oneline
  562  git log -3
  563  git log -1
  564  git log --oneline -2
  565  clear
  566  ls
  567  git log
  568  git log --oneline -2
  569  git show 5dda263
  570  clear
  571  clear
  572  cd ..
  573  ls
  574  mkdir samplerepo3
  575  clear
  576  cd samplerepo3/
  577  clear
  578  ls
  579  echo "rec1" >> f1.txt
  580  git add .
  581  clear
  582  git init
  583  echo "rec1" >> f1.txt
  584  git add .
  585  git commit -m "cm1"
  586  clear
  587  echo "rec1" >> f2.txt
  588  git add .
  589  git commit -m "cm2"
  590  echo "rec1" >> f3.txt
  591  git add .
  592  git commit -m "cm3"
  593  echo "rec1" >> f4.txt
  594  git add .
  595  git commit -m "cm4"
  596  git log --oneline
  597  cd ..
  598  cd samplerepo2/
  599  git log --oneline
  600  clear
  601  git log --oneline
  602  ls
  603  git ls-files
  604  git status
  605  git reset --soft a8de804
  606  git status
  607  ls
  608  git ls-files
  609  git log --oneline
  610  git status
  611  git commit -m "cm5.1"
  612  git status
  613  ls
  614  git ls-files
  615  git log --oneline
  616  git reset --mixed a8de804
  617  git ls-files
  618  git status
  619  git log --oneline
  620  git add .
  621  git commit -m "cm5.2"
  622  clear
  623  ls
  624  git ls-files
  625  git status
  626  git log --oneline
  627  git reset --hard a8de804
  628  git status
  629  ls
  630  git ls-files
  631  git log --oneline
  632  git reset --hard f8fdbc9
  633  git log --oneline
  634  git reset --hard 58bc9cf
  635  ls
  636  git status
  637  ls
  638  git ls-files
  639  git log --oneline
  640  clear
  641  ls
  642  echo "rec1" >> l1.txt
  643  git add .
  644* git commit -m "cm3"
  645  echo "rec1" >> l2.txt
  646  git add .
  647  git commit -m "cm2"
  648  echo "rec1" >> l3.txt
  649  git add .
  650  git commit -m "cm3"
  651  echo "rec1" >> l4.txt
  652  git add .
  653  git commit -m "cm4"
  654  clear
  655  git log --oneline
  656  ls
  657  git revert 5ae221e
  658  ls
  659  git ls-files
  660  git status
  661  git log --oneline
  662  git show 5c9e9c6
  663  git log --oneline
  664  git revert 5c9e9c6
  665  ls
  666  git ls-files
  667  git status
  668  git log --oneline
  669  clear
  670  git log --oneline
  671  ls
  672  clear
  673  ls
  674  cd ..
  675  ls
  676  mkdir samplerepo4
  677  cd samplerepo4/
  678  clear
  679  git init
  680  echo "rec1" >> f1.txt
  681  git add .
  682  git commit -m "cm1"
  683  clear
  684  git branch
  685  ls
  686  git log --oneline
  687  git branch feature1
  688  git branch
  689  git log --oneline
  690  git switch -c feature2
  691  git log --oneline
  692  ls
  693  git switch master
  694  clear
  695  ls
  696  git switch feature1
  697  ls
  698  echo "rec1" >> s1.txt
  699  ls
  700  git add .
  701  git commit -m "f1cm1"
  702  git log --oneline
  703  ls
  704  git switch master
  705  ls
  706  git log --oneline
  707  git merge feature1
  708  ls
  709  git log --oneline
  710  cd ..
  711  mkdir samplerepo5
  712  clear
  713  git init
  714  ls -a
  715  rm -rf .git/
  716  clear
  717  ls
  718  cd samplerepo5/
  719  clear
  720  ls
  721  git init
  722  git branch
  723  git status
  724  echo "rec1" >> file1.txt
  725  git add .
  726  git commit -m "cm1"
  727  clear
  728  ls
  729  cat file1.txt
  730  git branch feature1
  731  git branch feature2
  732  git branch
  733  git log --oneline
  734  git switch feature1
  735  ls
  736  cat file1.txt
  737  git log --oneline
  738  echo "Record2 from feature1" >> file1.txt
  739  git add .
  740  git commit -m "f1cm1"
  741  cat file1.txt
  742  git log --oneline
  743  git switch master
  744  cat file1.txt
  745  git log --oneline
  746  git merge feature1
  747  git log --oneline
  748  cat file1.txt
  749  git switch feature2
  750  git log --oneline
  751  cat file1.txt
  752  echo "Record2 from feature2" >> file1.txt
  753  cat file1.txt
  754  git add .
  755  git commit -m "f2cm1"
  756  git log --oneline
  757  git switch master
  758  git log --oneline
  759  cat file1.txt
  760  git merge feature2
  761  clear
  762  git status
  763  cat file1.txt
  764  vi file1.txt
  765  git add .
  766  git commit -m "Fixed Conflicts"
  767  clear
  768  cd ..
  769  mkdir samplerepo6
  770  clear
  771  cd samplerepo6
  772  git init
  773  clear
  774  echo "rec1" >> f1.txt
  775  git add .
  776  git commit -m "cm1
"
  777  echo "rec1" >> f2.txt
  778  git add .
  779  git commit -m "cm2
"
  780  echo "rec1" >> f3.txt
  781  git add .
  782  git commit -m "cm3
"
  783  echo "rec1" >> f4.txt
  784  git add .
  785  git commit -m "cm4
"
  786  clear
  787  git log --oneline
  788  git branch feature1
  789  git branch feature2
  790  git switch feature1
  791  echo "rec1" >> a1.txt
  792  git add .
  793  git commit -m "f1cm1"
  794  echo "rec1" >> a2.txt
  795  git add .
  796  git commit -m "f1cm2"
  797  echo "rec1" >> a3.txt
  798  git add .
  799  git commit -m "f1cm3"
  800  clear
  801  git log --oneline
  802  ls
  803  git rebase master
  804  git switch master
  805  git merge feature1
  806  git switch feature2
  807  ls
  808  git log --oneline
  809  echo "rec1" >> d1.txt
  810  git add .
  811  git commit -m "f2cm1"
  812  echo "rec1" >> d2.txt
  813  git add .
  814  git commit -m "f2cm2"
  815  clear
  816  git log --oneline
  817  git rebase master
  818  git log --oneline
  819  git switch master
  820  git merge feature2
  821  git log --oneline
  822  git switch feature1
  823  git log --oneline
  824  git rebase master
  825  git log --oneline
  826  celar
  827  clear
  828  ls
  829  git status
  830  git log --oneline
  831  echo "rec1" >> k1.txt
  832  git add .
  833  git commit -m "m1"
  834  echo "rec1" >> k2.txt
  835  git add .
  836  git commit -m "m2"
  837  echo "rec1" >> k3.txt
  838  git add .
  839  git commit -m "m3"
  840  echo "rec1" >> k4.txt
  841  git add .
  842  git commit -m "m4"
  843  echo "rec1" >> k5.txt
  844  git add .
  845  git commit -m "m5"
  846  clear
  847  ls
  848  git log --oneline
  849  git rebase master
  850  git switch master
  851  ls
  852  git log --oneline
  853  git merge --squash feature1
  854  git status
  855  git commit -m "Updated multiple changes from feature1"
  856  git log --oneline
  857  git status
  858  git switch feature2
  859  echo "rec1" >> w1.txt
  860  git add .
  861  git switch master
  862  git status
  863  git switch feature2
  864  git status
  865  git stash list
  866  clear
  867  git status
  868  git stash save "created from w1.txt"
  869  git stash list
  870  echo "rec1" >> v1.txt
  871  git add .
  872  git stash save "for v1.txt"
  873  git stash list
  874  echo "rec1" >> v2.txt
  875  git add .
  876  git stash save "for v2.txt"
  877  echo "rec1" >> v3.txt
  878  git add .
  879  git stash save "for v3.txt"
  880  echo "rec1" >> v4.txt
  881  git add .
  882  git stash save "for v4.txt"
  883  git stash list
  884  git status
  885  ls
  886  clear
  887  git stash list
  888  git stash apply
  889  ls
  890  git commit -m "v4"
  891  git status
  892  git stash list
  893  git stash drop
  894  git stash list
  895  history


		
#########################
Day - 4 | 8th Feb. 2026
#########################			 
		


	- git stash :::
		
		 - Used to maintain the uncommited changes to a temporary area.
		 
		 - Stash is List Data Structure 
		 
		 
		 
		 git stash list 
		 
		 git stash save "create from w1.txt"
		 
		 git stash apply 				# Apply the latest changes back to staging area 
		 
		 git stash apply stash@{3}		# Apply the specific change back to staging area 
		 
		 git stash drop 				# Delete the latest changes from the stash list
		 
		 git stash drop stash@{3}		# Delete the specific changes from the stash list
		 
		 git stash pop 					# Apply and delete the lastest changes
		 
		 git stash pop stash@{2}		# Apply and delete a specific change
		 
		 git stash clear 				# Clean-up the entire stash list
		 


	- github remote repos ::::
	
		- github :
		
		- gitlab/azRepos :
		
		
		Developers' Workload :
		
		- Enhancement/Bugfix Project
		- New Product development
		
		
		- git clone / git push / git fetch / pull
		
		
		
		- Pull Request :::
		
			- Created by the Developers 
			- It is used to merge the changes to main/master branch 
		
		
		- ML Project :
		
			- Tuning the Model 
		
		
		
		Git Cli Commands :::
			
			git clone 		-> To Copy/Clone the entire remote repository to Local Machine
			
			git add 		-> To Add the changes from working directory to staging area 
			
			git commit 		-> To Commit the changes from staging area to local repository
			
			git push		-> To Push the changes from local Repository to Remote Repository
		
			git fetch/pull :
			
				-> Both Git fetch and pull are used handle the incremental changes from remote repository.
				
				Git fetch :
				
					- It is used to just check for the incremental changes from remote repository, if any incremental changes exist, it will update the details of the changes only in local repository. Git fetch will never Update the Working Directory.
					
					- Do explicit merge to update the Working Directory					
					
			
				git Pull :
				
					- It is used to check for the incremental changes from remote repository, if any incremental changes exist, it will update the details of the changes in local repository as well as in the Working Directory.
					
					git pull => git fetch + git merge 
					
					To keep the local repo in sync with remote repo.
					
		
		- git remote -v
		
		- git clone <Repo_URL>
		
			- git clone https://github.com/Edu-MLOps-Feb26/TestRemoteRepo1.git		
		
		- git push 
		
			- git push -u origin localfeature1
			
			- Remote repo is needs token based authentication to push the changes 
			
			- Password based 
			
			- Token based : asdadasdao
			
	
		- git remote -v 			# To list the remote repos that are linked to local repo
			
		- git remote add origin https://github.com/Edu-MLOps-Feb26/TestRemoteRepo1.git		# To Add remote repo to local 
		
		- git remote remove origin 	# Remove remote repo link
		
		
	- VS Code :::
	
		-> Install VS Code 
		
		-> AWS Free Tier Account 
		
			- Login to AWS Account 
			
			- Create EC2 instance ==> Build Enviroment to Build the ML Project using git-actions CI
			

		
		
		
	- AWS Console :::

		- root user ID 
		
		- Password 
		
		- MFA 

			- Login to AWS Console as a root user 
			
			- Create IAM User user profile with Admin Access
			
		
			
		
		
			- AWS S3 Buckets -> To manage the ML Datasets 
		
	S3 Buckets :::
	
	
		--> Bucket = Directory
		
			--> Files -> Keys/ Objects
			
		--> Access to Create a S3 Bucket 	
		
		--> Create s3 Bucket 		--> loksai-edu-mlops-0208-bk

Set S3 Bucket Policy :
		
{
  "Version": "2012-10-17",
  "Statement": [
	{
	  "Sid": "Statement1",
	  "Effect": "Allow",
	  "Principal": "*",
	  "Action": [
		"s3:*"
	  ],
	  "Resource": "arn:aws:s3:::loksai-edu-mlops-0208-bkt/*"
	}
  ]
}		
	
		--> IAM Role to Access S3 Bucket using EC2 Instances (VM)	
		
		https://loksai-edu-mlops-0208-bkt.s3.ap-south-1.amazonaws.com/index.html
		
		
		
	Working with AWS Cli Commands to enable Process Automation :
	
	
	- Create IAM Role 		# It is used to give permission to EC2 Instance to access s3 Bucket 
							# loksai-edu-mlops-0208-role
							
	- Create EC2 Instance 	# loksai-edu-mlops-0208-Dev_Server
		-> Linux - ami: ubuntu 22.04v
		
		
		Authentication Methods/Types:
		
			- Password based Authentication
			- Token based Authentication		cli/api
			
			- Key Based Authentication
				- Public Key 
				- Private Key 
				
			- Passwordless Authentication
			
			
			Free-Tier Account --> 100$
			
			1 VM --> 750 Hrs/Month 
			
			10 VMs --> 75 Hrs/Month
	
	
	- Connect to EC2 Instance :
	
		-> EC2 Instance Connect 	==> Web Browser 
		
		-> SSH Agents :			# Windows Users 
			- Putty 
			- MobaXterm 
				--> Install MobaXterm Package
				https://mobaxterm.mobatek.net/download-home-edition.html
			
		-> Terminal/cmd				# Mac/Linux/Windows Users
				Open an Terminal.

				Locate your private key file. The key used to launch this instance is AWS_MLOps_KeyPair.pem

				Run this command, if necessary, to ensure your key is not publicly viewable.
				chmod 400 "AWS_MLOps_KeyPair.pem"

				Connect to your instance using its Public DNS:
				ec2-65-2-6-239.ap-south-1.compute.amazonaws.com		
				
				ssh -i "AWS_MLOps_KeyPair.pem" ubuntu@ec2-65-2-6-239.ap-south-1.compute.amazonaws.com
		
			Connection Parameters :
				Host_Name 			==> Public IP Address
				User_Name 			==> ubuntu
				Credential 
					- SSH Key ==> *.pem
			
		
Next ::

	-> ML Project :::
	
	-> MLOPS Workflow :
	
	-> Python --> 
	
		- Web Application			==> User Interface 
		- API 						==> Backend
		
		-> github, s3 bucket 
		
	-> Containerization :
	
		- Docker 
		
		- Kubernetes 



		
#########################
Day - 5 | 14th Feb. 2026
#########################	
		
		
	- Package the ML Models :::
	
		- Containerization using Docker.
		
		- API Call / Web Application
		
	- Build and Deploy ML Models
	
	- Automate using github-actions 
	
	- ETL --> 
	
	
	- Packing the ML Model ::::
	
		- Tools 
		
		- Projects 
		
		- Packaging methods 
	
	
	- Docker ::::
	
	
	ML Projects :::
	
		-> Basic Feedback form!
			- Packaging 
				- How to Package Docker
	
	
		-> Actual Project 
			- DVC,Streamlit,Fast-API,MLflow 
	
	
		-> Actual Project 
			- Sagemaker-ai
		
	
	Containerization ::::
	
		-> It is a process to package the application along with its' dependencies.
		
		
		Tool :
		
		Docker is one of the Containerization Tools
		

			
			
		What is Virtual Machine ?
			- VMs are called as Hardware level Virtualization 
			- VMs are created using  Hypervisor 
			- VMs are used to execute Operating System 
			- VMs will continue to execute, even if there is no active Task/Application.
			- VMs consume more time and space to start the Task/Application	
		
		What is Container ?
			- Container are called as OS level Virtualization 
			- Container are created using  Container Engine 
			- Container are used to execute Task/Application. Not Operating System 
			- Container will immediately go to exit state, if there is no active Task/Application.
			- Container consume less time and space to start the Task/Application	

			- Container uses tow important properties from base OS 
			
				- Kernel - Core of the OS.
				
					--> Control Group					
					--> Namespace
		
		Setup Docker Engine on EC2 Instance :::
		

	Use-cases of Docker :::
		
		- Infra-Structure Perspective ::
		
			Build Enviroment (VM) -> Install all the required tools. 500 Enviroments
			
				Docker Containers are used to reduce the no. of VMs.			
		
			Build Enviroments:
			
				- Java Project (VM) 
				- Python (VM)
				- .Net (VM)
				- Angular (VM)
				- NodeJS (VM)
				- Ruby (VM)				
				
			Virtual Machine (VM) - Build Server 
				- Install Container Engine 
					- C1 	- Java Project (VM) 
					- C2    - Python (VM)
					- C3    - .Net (VM)
					- C4    - Angular (VM)
					- C5    - NodeJS (VM)
					- C6    - Ruby (VM)					
					
			- This refered as server templates. 
			
		
		
		- Developers/Deployment Perspective ::
		
			Java Project (VM)
			
			 - Create source code 					-> Source code mywebapp.java
			 - build 								-> Create Artifacts - (mywebapp.war)
			 - unit test
							mywebapp.war
							Tomcat Web Application Server -> jdk17,tomcat8.5
					
						Package the application along with its' dependencies.
 
					Docker : Create a package/Image ==> mywebapp:v1.0(mywebapp.war,jdk17,tomcat8.5)
					
					Publish the Image to Container Registry -> DockerHub
				
			 - promote the change to test environment
			 
				
				QA Envi: 	(mywebapp.war)
					Pull the application image from dockerhub and create container out of it.
				
						mywebapp:v1.0
						
				
				UAT Envi: 	(mywebapp.war)
				

				PROD Envi: 	(mywebapp.war)
				
		
		Terminologies ::
		
			- Container Engine: 	- It is a Tool to Manage the Container Images and Containers
									- Eg.: Docker Container Engine.
									
			- Image					- Is a static file,which defines the properties of the application to create container
									- Non-Executable 
									- Images are create using the Dockerfile Instructions based of incremental layers.
			
			- Container				- Executable units of Container image.
			
			- Container Registry	- Used to maintain/version control the container images
									- DockerHub 
									
									
						Eg.: github 	- To Version Control Source Codes 
							 dockerhub 	- To Version Control container images
							 
			- Container Repository	- Sub-set of Container Registry
									- Can be created based on App Teams/Enviroments 
									
	
		Work with Docker :::
		
			- Launch EC2 Instance - Linux Ubuntu 22.04 version .
			
			- Login to EC2 Instance 
			
			- Install docker 
			
				- sudo -i 
				- apt install docker.io			# https://docs.docker.com/engine/install/ubuntu/
				
			- Use Docker CLI Commands to interact with Docker Engine to create/Manage the containers/Images.
			
			
			
		
#########################
Day - 6 | 15th Feb. 2026
#########################		
		
	- Work with DockerHub - Account/Repos		# https://hub.docker.com/
	
	- Install Docker Engine 
		- Launch EC2 Instance - Ubuntu AMI v22.04
		
		- Connect using SSH Agent 


		sudo -i 

		apt update -y 

		apt install docker.io -y				# Install Docker



	- Work with Docker Cli Commands 
	
		--> Use Existing Image from DockerHub
	
			docker images 			# To get the list of images on the local machine

			docker ps 				# To get the list of active containers on the local machine	

			docker ps -a 			# To get the list of active & inactive containers on the local machine	
			
			
			docker pull ubuntu 		# Download an Container Image from Docker Hub
			
				- Syntax: 	docker pull <image>:<tag/version_ID>
							docker pull <image>							# Always refer to the latest version
	
	
			docker run <image_name>	# To Create a New Container based on the Image Name.
									# Run/Execute the Container
			
			
			Modes of Execution :::

				- Attached Mode / Foreground Mode 		# Default
				
					Eg.: docker run ubuntu sleep 5
				
				- Detached Mode / Background Mode 
				
					Eg.: docker run -d ubuntu sleep 5
				
				- Interactive Mode :::
				
					Eg.: docker run -it ubuntu bash
				
		
			docker start <container_id>		# Start the Container 
			
			Login to running Container :
			
				- docker exec -it <container_id> bash 	# To Login to running container
			
				- exit 									# To Logoff from the running container 

			docker stop <container_id>		# Stop the running Container 
			
		
		Working with Web Servers with-in Docker Container.
		
		Docker Networking :
		
		Port Mapping / Port Binding :::		# To expose the container application to internet
		
		
			Syntax : -p <host_port>:<container_port>
			
			Eg.: 
				docker run -it -p 8080:8080 tomcat:8.0
			
				docker run -it -p 8089:8080 tomcat:8.0
			
			
		Access the Container application using the <External_IP/Public_IP>:<host_port>
		
	
		Remove Container :
		
			docker rm <container_id>
		
		Remove Image :
		
			docker rmi <image_id>
			
	
	--> Create New Images ::::
		
		- Docker Commit ::	# Infra-Structure perspective
		
			- It used to create a new Container Image based on the existing Container reference
			
			Syntax: 
			
				docker commit <container_id> <DockerHub_Repository_Name>/<New_Container_Image>:<tag/version_ID>
			Eg.: 
				
				docker commit 922ee3d65714 loksaieta/tempimg:v1.0
				
		
		
		
		
		- Docker Build ::	# Development/Deployment Process 
		
			- It used to create a new Container Image based on the Dockerfile reference
			- Dockerfile composed of Dockerfile Instructions 
		
		
			
		vi Dockerfile 
		
		FROM ubuntu
		RUN apt update -y 
		RUN apt install git -y 
		
			- 

			Syntax: 
			
				docker build -t <DockerHub_Repository_Name>/<New_Container_Image>:<tag/version_ID> .		# '.' denotes Dockerfile reference
			Eg.: 
				
				docker build -t loksaieta/dummyimg .			
			
			
	
		- Publish the Container Image to Container Registry ::
		
			- Login to Container Registry using Docker CLI Commands 
			
				- DockerHUB Login_ID
				
					docker login -u loksaieta
				
				- PAT - (Password)
				
					asdasdasd
					
					
			- Push the Container Image to DockerHub :
			
				docker push <DockerHub_Repository_Name>/<New_Container_Image>:<tag/version_ID>
		
				Eg.: 
				
					docker push loksaieta/tempimg:v1.0
					
			
	
		Dockerfile Instructions :::
		
		
			FROM 				# To identify the base Image 
			
			RUN					# To Run/Execute and Package 
		
			COPY				# To Copy the files from Host Volume to the Container Volume 
		
			CP					# To Copy the files within the Container Volumes 
			
			ADD 				# To Copy the files from external URL/Host Volume to the Container Volume 
			
			WORKDIR				# To set the current working directory within the Container 
			
			ENV 				# To set the Enviroment Variable
			
			EXPOSE				# Set the Container Port 
			
			CMD					# To set the Start-up task/Command for the Container 
								# This can be modified at run time 
								
			ENTRYPOINT			# To set the Start-up task/Command for the Container 
								# This cannot be modified at run time 				


		
		
	
	Container Orchestration using Docker :::
	
		- Docker-compose		# To run Multiple containers as service 
		- Docker Swarm 
	
	Container Orchestration using Kubernetes :::
	
		- Kubernetes is an Open-Source Container Orchestration Tool 
		- To Ensure High availability of the Container and Applications
		- It creates replicas of Containers/Pods 
		- Kubernetes support Auto-Scaling and Load Balancing 
		- Self-healing 
		- Upgrade/Downgrade of application services can be done without any downtime
		- Using Deployment Strategies : Rolling-Update Strategy 
	
	Product: 
	
		mymlproject:v1.0  --> LIVE Version 
		
		mymlproject:v2.0  --> How to upgrade without any downtime ?
	
	
		
			
		
	- 3 tier application :
	
		front-end layer			c1,c1,c1 
			
		application layer 		c2,c2,c2 
		
		database layer			c3,c3,c3 
		
	
Next :

		Build ML Model 
		
		DVC/MLFlow/Streamlit/Fast-API
		
		S3 Integration 
		
		DockerHub Integration 
		
		Deployment of Kubernetes 
			




#########################
Day - 7 | 21st Feb. 2026
#########################	


	- ML Project --> Overview 
	
	- DVC/MLFlow/Streamlit/Fast-API
	
	- Create - AWS S3 Bucket	# To maintain the Dataset & Metadata
	
	- Docker Cli & DockerHub 
	
	- Kubernetes -
	
	- Github :
	
		- Maintain the ML Project Source Code 
		
		- github-actions --> Enable CICD 
		
		
	Enviroments :
	
		-> Dev Enviroment -> IDE - VSCode / Editor
		
		-> Build Enviroment -> Build ML Project -> Docker Images -> Publish to DockerHub
		
		-> Target Enviroments : ==> Kubernetes Cluster
		
			- UAT	 
			
			- Prod Enviroments
			
	
	Dataset Management :

		-> AWS S3 / Azure Disk / G Persistant Volumes / External Storage Servers
		
		-> 

			1. Raw Data Extraction 				==>  raw data s3/ --> for processing
			
			2. Process Data --> v1,v2,v3,v4,v5	==> s3 
			
			3. Train the Model 
			
			4. Test the Model 	-> Experiment --> 
			
				-> Feedback		--> The Accuracy of ML Model will be better with more data. --> 95% 98% 
				
								--> Monitoring Tools :
								
									--> MLFlow == To perform ML Project Experiments & verify the Model Accuracy
									
									--> Infra-Structure Monitoring :
									
										- Prometheus/Grafana --> 
								
			5. AWS Cli --> to interact with S3 Bucket 
			
			6. SDK --> Python program -> boto3 Library  
			
			
			
	- Version Control System :

		- git 			# To Version control the Source Code (*.py -> UI, API)
		
		- dvc 			# To Version Control the Datasets and Process.
		
						# DVC Pipeline :
						
							Process the data 
							
							Train the model
							
		- S3 			# To Version Control the Processed Datasets and metadata.
		
		- DockerHub 	# To Version Control the Application Images 
		
		
	
	Managed Services : AWS Sagemaker-ai / Azure / GCP
		
	
	DVC ::::	Data Version Control::		# https://dvc.org/  # https://doc.dvc.org/start
	
		==> Maintain the Version of ML Datasets.	


		--> Install DVC ???
		
			- Prepare Virtual Enviroment 
			
				-> Install dvc/mlflow/streamlit/fast-api
	
	
	Launch EC2 Instance ::: 
	
		- Ubuntu AMI 22.04v with IAM Role(Access s3) -> 

		- Connect to EC2 Instance
				
				- Install :	AWS CLI, Docker Cli, git
				
				- Install : Python3, Python3 environment pkg
			
		

Login to EC2 Instance :::::

sudo -i 

1 Update system

sudo apt update -y


# To Setup AWS CLI ::::				https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html


sudo apt install -y unzip curl

2 Download AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

3 Unzip installer
unzip awscliv2.zip

4 Install AWS CLI
sudo ./aws/install

5 Verify Installation
aws --version


Expected output:

aws-cli/2.x.x Python/3.x Linux/x86_64


 AWS CLI installed successfully


# To Setup EC2 Instance with AWS ::::




1.2 Install Required Tools

sudo apt update -y 

sudo apt install -y \
  python3.12 python3.12-venv python3-pip
  
  

1.3 Configure AWS CLI

AWS Resources can be accessible using AWS Cli :::

- It is always recommended to use IAM Role to access AWS resources from EC2 Instance.

- AWS Access Key & Secret Key approach is not recommended as per the security policy.


	aws configure		# To Configure AWS CLi using AWS User Access Key & Secret Key. 

	Goto to Security Credential tab in IAM Console to create Access Key & Secret Key : 

	AWS User Access Key:  asdfsdf

	AWS User Secret Key:  asdf

	AWS Region : ap-south-1
	
	
Prepare Virtual Enviroment :::

Examples: 

	python3 -m venv venv
	source venv/bin/activate	

deactivate 

	python3 -m venv loksai_env
	source loksai_env/bin/activate

deactivate 

# install dvc Package within the Venv.

pip install dvc_s3									# Install DVC-s3 

dvc --version

	
dvc repro 			# 


Prepare a ML Project ::

git init 

dvc init 

git remote add <github_url> origin

git remote -v


dvc remote add -d myremote s3://loksai-edu-mlops-0208-bkt

dvc remote list 

git add .

git commit -m "Initialize DVC with S3"


vi "Hello" >> raw_dataset.csv

dvc add raw_dataset.csv


DVC Pipeline :
	- Stage1: Process data 
	
	- Stage2 : Train the Model (Build)
	
		--> Output --> Artifacts  ==> pickle / model.joblib


dvc pipeline :

	dvc repro 		# Reproduce 
	
	
Workflow ::::	

	1. DVC is used to manage the Input Dataset for Model Training 
	
	2. dvc repro 		# Reproduce  ==> Train the Model 

	3. Trained Model ==> ML Artifacts ==> model.joblib / pickle 
	
	4. Used DVC to maintain the Artifacts - S3 
	
	5. Use that artifacts and dependencies to create Dockerfile and create Docker Image 
	
	6. Publish Docker Image to Container Registry
	
	7. Deploy to Kubernetes 
	

S3 :

	Upon training the Model, 
	
	dvc push to push the changes to s3.



dvc push 

dvc status 

dvc pull 


git (vs) dvc 	????



#########################
Day - 8 | 22nd Feb. 2026
#########################	


	- Onboard Data Science/ML Project to MLOps practice :::
	
		- Enable CICD Automation ::
		
			-> Build the Project 
			
			-> Create Container Images 
			
			-> Deploy to Kubernetes Cluster 
			
			-> Build Orchestration Tool: github-actions - to automate
			
				- Jenkins(Groovy)/Github-Action(*.yaml)/Azure_Pipelines(*.yaml)
			
				- Stages --> Task 
				
				Application Build & Deployment :
				
					- SCM_Checkout	-> 
					- Application Build --> Create Artifacts 
					- Application Image Build 
					- Publish to Container Registry(DockerHub)
					- Deploy to Target Enviroment(Kubernetes)
					
				

git init 

dvc init 

dvc remote add -d s3remote s3://edu-loksai-mlproj-bkt1

git add .dvc/config

git commit -m "Configure DVC S3 remote"

vi data/raw/reviews.csv				# Add sample Records as shown below		

dvc add data/raw/reviews.csv		# Add data/raw/reviews.csv to git after every dvc add

git add data/raw/reviews.csv.dvc data/raw/.gitignore

git commit -m "Added DVC Data-v1"

dvc repro

git add data/processed/.gitignore models/.gitignore dvc.lock			# Add dvc.lock to git after every dvc repro

git commit -m "repro-Processed data-v1"

dvc push 							# Push repro-Processed data 

dvc status							# Status should be: Data and pipelines are up to date.				



Containerize the ML Project - API & UI 

	- Docker Image -> ML Project - API & UI 
					-> Should not have any Trained Model
						- This Trained Model should received at runtime


use the Docker.api & Docker.ui   # Dockerfiles to Build Image.

	docker build -f docker/Dockerfile.api -t loksaieta/mlapi222 . 

	docker build -f docker/Dockerfile.ui -t loksaieta/mlui222 . 
		
		
		http://13.234.29.126:8071/docs			# Access Fast API
		


Automate using git-actions :::




Monitoring :::

			- MLFlow 
			- Prometheus/Grafana 
		
# Create data/raw/review.csv 


review_text,sentiment
I love this product it works perfectly,positive
Absolutely fantastic quality and service,positive
Very happy with the purchase,positive
Great experience would buy again,positive
Excellent product highly recommended,positive
Amazing performance and value,positive
Satisfied with quality and delivery,positive
Works exactly as advertised,positive
Customer support was very helpful,positive
This is the best purchase I have made,positive

This product is okay not great not bad,neutral
Average quality nothing special,neutral
It works but has some issues,neutral
Neither good nor bad experience,neutral
Product meets basic expectations,neutral
Decent but could be improved,neutral
Acceptable quality for the price,neutral
Nothing impressive but usable,neutral
Fine for occasional use,neutral
Okayish experience overall,neutral

Worst product I have ever bought,negative
Very disappointed with the quality,negative
Terrible experience waste of money,negative
Does not work as expected at all,negative
Extremely poor build quality,negative
Bad experience will not recommend,negative
Product stopped working in two days,negative
Customer service was terrible,negative
Totally useless and overpriced,negative
Highly dissatisfied with this product,negative

I am happy with the fast delivery,positive
Packaging was good and product works well,positive
Average packaging but product is decent,neutral
Late delivery and damaged box,negative
The item arrived broken,negative
Very smooth and reliable performance,positive
Not worth the price paid,negative
Okay product but slow response,neutral
Loved the design and usability,positive
Disappointed with the overall experience,negative

	
mkdir dir1 

git init 

dvc init 

dvc remote add -d myremote s3://bkt



dvc push 


deactivate 

dvc pipeline :

	dvc repro 		# Reproduce 

	1. DVC is used to manage the Input Dataset for Model Training 
	
	2. dvc repro 		# Reproduce  ==> Train the Model 

	3. Trained Model ==> ML Artifacts ==> model.joblib / pickle 
	
	4. Used DVC to maintain the Artifacts - S3 
	
	5. Use that artifacts and dependencies to create Dockerfile and create Docker Image 





		
			
	MLOPs based on DevOps ==> CICD Pipeline using git-actions
	
pip install --upgrade dvc dvc-s3 mlflow 
	

			
git init 

dvc init 

dvc remote add -d s3remote s3://loksai-edu-mlproject1  

git add .dvc/config

git commit -m "Configure DVC S3 remote"

vi data/raw/reviews.csv				# Add sample Records as shown below		

dvc add data/raw/reviews.csv		# Add data/raw/reviews.csv to git after every dvc add

git add data/raw/reviews.csv.dvc data/raw/.gitignore

git commit -m "Added DVC Data-v1"

dvc repro


git add data/processed/.gitignore models/.gitignore dvc.lock			# Add dvc.lock to git after every dvc repro

git commit -m "repro-Processed data-v1"

dvc push 							# Push repro-Processed data 

dvc status							# Status should be: Data and pipelines are up to date.			
			
								
		
Finally Commit the entire project source code and push it to github repository.		

git add .

git commit -m "Created ML-Project_V1"	

git push -u origin main


https://github.com/Edu-MLOps-Feb26/mlops-e2e-project.git
	
		
		
Build Environment ::::

	--> DEV/TEST/PROD - VM 
	
	- build environment 
	
	--> github-actions / azure pipelines  / gitlab-ci
			
	
			- Build Server: :
			
				- Static 
					- default server/runner - fully managed by github
					- it gets deleted automatically 
					
					-> Artifactory Libraries / DockerHub / S3 
					
					
					
				- Dynamic ::
				
					- Self-Hosted runner : used to build/deploy -> Target Enviroments 
					
				
	--> Kubernetes Cluster acts as a self-hosted agent :::
	
		Create Kubernetes Cluster :::		
			
		Container Orchestration using Kubernetes :::
		
			- Kubernetes is an Open-Source Container Orchestration Tool 
			- To Ensure High availability of the Container and Applications
			- It creates replicas of Containers/Pods 
			- Kubernetes support Auto-Scaling and Load Balancing 
			- Self-healing 
			- Upgrade/Downgrade of application services can be done without any downtime
			- Using Deployment Strategies : Rolling-Update Strategy 

				
		Concepts :
		
			- Kubernetes Architecture & its Component 
			
			- Kubernetes Terminologies 
			
			- Kubernetes Components 

				- pods 
				- deployments
				- replicasets 
				- services 
					- NodePort Service 
					
			- Manifest file 
	
To Setup the SECRET on github actions :::	
	
	docker login -u loksaieta
	
	sdfsewe
	
	
	
	Build Enviroment :
	
		- Create app image & Published to DockerHub 
		
		
		
	Target Enviroments :
	
	
	
	
		- QA 
		
		- UAT 
		
		
		
		- PROD -- server1,2,3,4,5,.............,100  -- High available 
		
		
		Docker -- Container 
		
		Kubernetes --> Containers are executed as Pods 
				   --> A Kubernetes Pod is a Logical unit of schedule runs on kubernetes
				   
				   
		-> Kubernetes :
		
			Kubernetes Master :
			
				Kubernetes WorkerNodes :
				
				
		- API Server 
		
		- ETCD 
		
		- Scheduler 
		
		- Controller Manager 
		
		- Kubelet 
		
		- Kube-proxy 
		
		- CRI Container Runtime Interface : Container-D 
		

Next ::

	Setup the Kubernetes Cluster 
	
		- Single Node K8s 
		
	- Deploy pods 
	
	- Integrate Kubernetes Cluster as a self-hosted Runner on Github actions 
	
	- Monitoring 
	
		- mlflow 
		
		- Prometheus/Grafana
			
	- AWS Sagemaker => Fully Managed Paid Service. 



	
	
#########################
Day - 9 | 28th Feb. 2026
#########################	


	Build & Deployment :::
	
		- 
		
	github-actions :::
	
		- Build 		-> 
				-> Mlflow 
				
				
		- Deployment 	-> using Kubernetes
		
		
	Kubernetes -> 
	
		--> Setup Architecture 
		--> Core Concepts 
				- Pods
				- Pod Networking
				
				- Controller Objects 
					- Deployment 
					- Replicasets 
				- Namespaces 
				- Kubernetes Services 
		
	Integrate with git-actions 
	
		- Kubernetes Cluster acts as a self-hosted runner in git-actions
		
	
	Kubeflow :::
	
	AWS Sagemaker-ai - Fully Managed Service 
	
	Monitoring :
	
		- Infra-Structure Monitoring 
		
		- Model Monitoring 
		
		
		
		
	- Kubectl is a command line utility to interact with Kubernetes Master 
	
	- Kubernetes Manifest file --> *.yaml / *.json 
	
		- Developers Create Manifest file and maintain in the git repository
		
		
		
	- Install and Configure Kubernetes ::::
	
		- Kubernetes Architecture :
		
			- Kubernetes_Master 
			
				- Kubernetes_WorkerNode1
				- Kubernetes_WorkerNode2
				- Kubernetes_WorkerNode3
				- Kubernetes_WorkerNode4
				
		
	
		
			- Kubernetes_Master 				
			
				- Kubernetes_WorkerNode1		
				- Kubernetes_WorkerNode2
				- Kubernetes_WorkerNode3
		

			- Kubernetes_Master 							--> 				
			
				Kubernetes_Cluser1							--> AWS Mumbai
					- Kubernetes_WorkerNode1		
					- Kubernetes_WorkerNode2
					- Kubernetes_WorkerNode3
		
	
			- Kubernetes_Master 							--> 				
			
				Kubernetes_Cluser1							--> AWS Mumbai
					- Kubernetes_WorkerNode1		
					- Kubernetes_WorkerNode2
					- Kubernetes_WorkerNode3		
		
				Kubernetes_Cluser2							--> AWS Tokyo
					- Kubernetes_WorkerNode1		
					- Kubernetes_WorkerNode2
					- Kubernetes_WorkerNode3		
		
		
			- Kubernetes_Master 								--> On-Prem				
			
				Kubernetes_Master1								--> AWS Cloud							
					Kubernetes_Cluser1							--> AWS Mumbai
						- Kubernetes_WorkerNode1		
						- Kubernetes_WorkerNode2
						- Kubernetes_WorkerNode3		
			
					Kubernetes_Cluser2							--> AWS Tokyo
						- Kubernetes_WorkerNode1		
						- Kubernetes_WorkerNode2
						- Kubernetes_WorkerNode3					
		
				Kubernetes_Master2 								--> Azure 
					Kubernetes_Cluser1							--> Azure Mumbai
						- Kubernetes_WorkerNode1		
						- Kubernetes_WorkerNode2
						- Kubernetes_WorkerNode3		
			
					Kubernetes_Cluser2							--> Azure Tokyo
						- Kubernetes_WorkerNode1		
						- Kubernetes_WorkerNode2
						- Kubernetes_WorkerNode3		
		
		
	
	
	- Open-Source Kubernetes :::
	
		- Kubernetes Admin to setup this Architecture
		
			-> k3s, minikube, kind	==> Small scale poc/learning 
										Enable Single Node Cluster 
			
			-> kubeadm 				==> Real Production level implementations 
										Enale Multi Node Cluster 
										Installation & Configurations
			
	
	
	- Fully Managed Kubernetes Services :
	
		AWS 	: ECS,ECR,EKS
		
		Azure 	: ACS,ACR,AKS
		
		GCP 	: GCE,GCR,GKE
		
		
		
	
		
mlflow server \
--backend-store-uri sqlite:///mlflow.db \
--default-artifact-root s3://loksai-edu-mlproject1 \
--host 0.0.0.0 \
--port 5000


uvicorn api.main:app --host 0.0.0.0 --port 8000


http://<EC2_PUBLIC_IP>:8000

### Run Streamlit
streamlit run ui/app.py --server.address 0.0.0.0		


http://<EC2_PUBLIC_IP>:8501


Created Docker Images for API & UI 

Published to DockerHub 


Deploy to Kubernetes :::


		K3s			# https://docs.k3s.io/installation
		
		Setup Kubernetes Sigle Cluster & Update it as a github-actions self hosted runner
		
		- Launch EC2 Instance 
		
		
		- Install K3s 
		
		Github-Actions :::::
	
	Automate the Model Creation and Containerization using CICD Workflow - github-actions workflow.
	
	Setup-self-hosted Agent on github
	
	Deploy using Kubernetes 
	
		1. Launch a Target Server / Kubernetes Cluster & Attach IAM Role for S3.
		2. Open all the required ports 
		3. Login and Install the required packages
				- Docker / K3s/Minikube/Kubeadm 


			Use AWS Ubuntu - 22.04V AMI 
			
			- sudo -i 
			
			- apt update -y  
			
			- curl -sfL https://get.k3s.io | sh -
			
			- sudo systemctl status k3s
			
			- kubectl get nodes 

			
				- Install unzip utility / curl 
				
				- Install aws-cli 
				
sudo -i

apt install unzip curl

 METHOD 1  Install AWS CLI v2 (Recommended)

AWS CLI v2 is the latest, stable version.

 Step 1  Download the installer
cd /tmp
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

 Step 2  Unzip
sudo apt install -y unzip
unzip awscliv2.zip

 Step 3  Install
sudo ./aws/install

 Step 4  Verify
aws --version


You should see something like:

aws-cli/2.18.x Python/3.x.x Linux/x86_64

					
		4. Setup the Credentials:
			- Create linux user - devopsadminuser1
				- valid Credentials - 
					 Recommended to have ssh-key - public_Key & Private Keys 

#Add User : 

useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin

su - devopsadmin

#ssh-keygen

#for Ubuntu ::
#ssh-keygen -t rsa -b 2048 -m PEM								#ubuntu 20.04

ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				


ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public


#cat id_rsa.pub > authorized_keys

cat id_ecdsa.pub > authorized_keys

chmod 600 /home/devopsadmin/.ssh/*

					 
				- Enable user access to kubernetes Master 
				
					- Create .kube/config under user's home dir. 
					- Use this user's kube/config file content as token for Authentication on github runner.
					
					
					cat /home/devopsadmin/.kube/config 
					
		5. Setup github self-hosted action-runner
		
			- Goto the Repo level setting 
			- Actions -> Runner 
			- Create Runner based on the OS & follow the instructions. 
			
			
			Kubernetes_K3s :
			
			NodePort Service :  30000 - 32767 
			SSH port: 22 
			
			
			Build_Server: 
			
			ssh : 22 
			mlflow/streamlit/fast-api -> 5000,8000,8501
			
			
			Setup the Load Balancer:
			
Next :::

	Kubeflow on Kubernetes 
	
	AWS Sagemaker - ai 
	
	Monitoring 
	
		- Prometheus/Grafana
		
		
	
